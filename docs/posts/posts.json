[
  {
    "path": "posts/2021-10-23-on-lincoln-index/",
    "title": "On Lincoln Index",
    "description": "On your mark, release, recapture.",
    "author": [
      {
        "name": "Farhan Reynaldo",
        "url": "https://weaklyinformative.com"
      }
    ],
    "date": "2021-10-23",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nSimulation\nConclusion\n\nIntroduction\nSuppose you are writing a blog post, and you wanted to make sure there are no errors in the writing. You asked your colleague to review the blog post, and they found numerous errors. But, you can’t be sure if your colleague managed to spot all the errors. By asking your other colleague, you can estimate the number of errors even though they do not catch all the errors. How could that happen? This method is called the Lincoln Index. The Lincoln Index estimates the number of errors as:\n\\[\n\\text{expected errors} = \\frac{E_1 E_2}{S}\n\\]\nwhere \\(E_1\\) and \\(E_2\\) are the number of errors found by reviewer 1 and reviewer 2, \\(S\\) is the number of common errors found by both reviewers. Let’s say from the story above, the first reviewer found 25 errors, the second reviewer found 15 errors, and they both found 5 errors in common. So, the number of errors is estimated around \\(25 \\times15 / 5 = 75\\) errors. If you feel a little bit skeptical about the method, we could always resort to simulation to check if the method gives a reasonable estimate.\nSimulation\nLet’s assume that we know the \\(N\\) true number of errors in the blog post, which is 1000 errors. Instead of the number of errors found, each reviewer has \\(p_1\\) and \\(p_2\\) probability of finding errors. In this case, let’s set the \\(p_1 = 30\\%\\) and \\(p_2 = 20\\%\\).\n\nTRUE_BUG_COUNT = 1000\np1 = 0.3\np2 = 0.2\n\nFor each iteration, we simulate whether the reviewer catch the error, denoted as p1_errors and p2_errors. And then, we count the common errors both reviewers found. Finally, we plug the numbers into the equation above. The code is written as:\n\nfrom random import random, seed\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")\n\n\ndef is_found(p):\n    return 1 if random() < p else 0\n\n\ndef simulate(true_error_count, p1, p2, iterations=10_000):\n    estimated_error_counts = []\n    for _ in range(iterations):\n        p1_errors = [is_found(p1) for _ in range(true_error_count)]\n        p2_errors = [is_found(p2) for _ in range(true_error_count)]\n        common_errors = [\n            p1_error & p2_error\n            for p1_error, p2_error in zip(p1_errors, p2_errors)\n        ]\n        estimated_error_count = (\n            sum(p1_errors) * sum(p2_errors) / sum(common_errors)\n        )\n        estimated_error_counts.append(estimated_error_count)\n    return estimated_error_counts\n\nAnd as we pressed the simulate button, we found that the mean for the estimate bug is centered around 1000, our true number of errors. By simulating the data generating process, we also get the uncertainty on the number of errors.\n\n\nShow code\nfrom statistics import mean, stdev\n\nseed(12)\ncounts = simulate(TRUE_BUG_COUNT, p1, p2)\nprint(f\"Mean: {mean(counts):.2f}\\n Std: {stdev(counts):.2f}\")\nMean: 1008.58\n Std: 101.01\n\n\n\nShow code\nplt.hist(counts);\nplt.show();\n\n\nConclusion\nThe Lincoln Index assumes that every error has an equal chance of being spotted. Moreover, if the probability of spotting the errors is low, then the denominator is more likely to be zero since the probability for the common errors is also low. In conclusion, the Lincoln Index is merely an estimate but a useful one.\n\n\n\n",
    "preview": "posts/2021-10-23-on-lincoln-index/on-lincoln-index_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-10-23T22:06:49+07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-06-27-on-shower-problem/",
    "title": "On Shower Problem",
    "description": "Cause you're hot then you're cold, You're wrong when it's right.",
    "author": [
      {
        "name": "Farhan Reynaldo",
        "url": "https://weaklyinformative.com"
      }
    ],
    "date": "2021-06-27",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nExperiment\nConclusion\n\nIntroduction\nI saw this shower problem first from Cameron Davidson-Pilon’s reply on Allen Downey’s tweet. I was hooked to solve the problem using similar approach from Allen’s USB connector problem. But later on he posted his approach to the shower problem. The original problem posted by Chris Said stated as follows:\n\nThe Shower Problem\nHere’s the setup: You’re at your friend’s place and you need to take a shower. The shower knob is unlabeled. One direction is hot and the other direction is cold, and you don’t know which is which.\nYou turn it to the left. It’s cold. You wait.\nAt what point do you switch over to the right?\nLet’s make this more explicit.\nYour goal is to find a policy that minimizes the expected amount of time it takes to get hot water flowing out of the shower head. To simplify things, assume that the water coming out of the head is either hot or cold, and that the lukewarm transition time is effectively zero.\nYou know that the shower has a Time-To-Hot constant called \\(\\tau\\). This value is defined as the time it takes for hot water to arrive, assuming you have turned the knob to the hot direction and keep it there.\nThe constant is a fixed property of the shower and is sampled once from a known distribution. You have certain knowledge of the distribution, but you don’t know \\(\\tau\\).\nThe shower is memoryless, such that every time you turn the knob to the hot direction, it will take \\(\\tau\\) seconds until the hot water arrives, regardless of your prior actions. Every time you turn it to the cold direction, only cold water will come out.\nI don’t know how to solve this problem. But as a starting point I realize it’s possible to keep track of the probability that the hot direction is to the left or to the right. In the animation above, the probability that the hot direction is to the right is just the unexplored white area under the right curve, divided by the total unexplored white area of both curves.\nBut how do you turn that into a policy for exploring the space? Does anybody know?\n\nIn this post, I am going to approach this problem using a simple rule as baseline (or think of this as an “optimized” baseline).\nFirst, let’s assume we start from the left direction. After waiting for t seconds, the water is still cold. So, we try the right direction and wait longer than our previous waiting time. We switch the direction each time until we passed the Time-To-Hot constant (\\(\\tau\\)). But, how long should we wait on the next try? Since the author provides the additional information that the distribution of \\(\\tau\\) is Weibull with parameters \\(\\lambda=50\\) and \\(k=1.5\\), we would sample the additional waiting time from that distribution. Furthermore, our average duration should be reported using the provided \\(\\tau\\) samples.\nExperiment\n\nimport os\n\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport pandas as pd\n\n\nshower_url = 'https://gist.github.com/csaid/a57c4ebaa1c7b0671cdc9692638ea4c4/' \\\n             'raw/ad1709938834d7bc88b62ff0763733502eb6a329/' \\\n             'shower_problem_tau_samples.csv'\n\ntest = pd.read_csv(shower_url)\ntest.head()\n   direction        tau\n0        1.0  44.173094\n1        1.0  39.133131\n2        1.0   2.229446\n3        1.0  45.518857\n4        1.0  32.463695\n\nThe explanation above can be written as the following code, where we keep track on the number of flips and the time elapsed. Initially, we would set the init_wait = 0, meaning our waiting time would be equal to the Weibull distribution sample. For illustration purposes, we set the direction = 1 and tau = 50.\n\ndef simulate(direction, tau, init_wait, lam, k, flips=0, total_time=0):\n  samples = stats.weibull_min.rvs(1.5, scale=50, size=1000)\n  \n  while True:\n    wait = init_wait + samples[flips]\n    if direction and tau < wait:\n      return flips, total_time + tau\n    \n    # change direction\n    direction = not direction\n    init_wait = wait\n    total_time += wait\n    flips += 1\n\nsimulate(direction=1, tau=50, init_wait=0, lam=50, k=1.5)\n(0, 50)\n\n\ndef run_simulations(df, func, **kwargs):\n  res = []\n  samples = zip(df['direction'], df['tau'])\n  for direction, tau in samples:\n    flips, total_time = func(direction, tau, **kwargs)\n    res.append((flips, total_time))\n  return np.transpose(res)\n\nflips, total_time = run_simulations(test, simulate, init_wait=0, \n                                    lam=50, k=1.5)\nflips.mean(), total_time.mean()\n(1.26565, 115.3636726932822)\n\nBy running the simulation against the test samples, the average waiting duration is around 115 seconds. Our baseline is lower than the previous solution by Cameron Davidson-Pilon and Allen Downey, 111.4 seconds and 102 seconds respectively.\nBut, what if we set another value to the init_wait parameter? Since we knew the problem followed Weibull distribution, we might try various values between 5% and 95% percentile. We should also simulate against a train set, which we would create later, to find the optimal init_wait.\n\ntrain = test.copy().sample(1000)\ntrain['tau'] = stats.weibull_min.rvs(1.5, scale=50, size=len(train))\n\nlow, high = stats.weibull_min.ppf([0.05, 0.95], 1.5, scale=50).astype(int)\ninit_waits = list(range(low, high))\n\ntime = []\nfor init_wait in init_waits:\n  flips, total_time = run_simulations(train, simulate, init_wait=init_wait, \n                                      lam=50, k=1.5)\n  time.append(total_time.mean())\n  \nsns.regplot(x=init_waits, y=time, lowess=True)\n\n\nBased on the plot above, the optimal value is around 45. Let’s simulate the test set using our new init_wait.\n\nflips, total_time = run_simulations(test, simulate, init_wait=45, lam=50, k=1.5)\nflips.mean(), total_time.mean()\n(0.68935, 109.16092853908714)\n\nConclusion\nIn conclusion, the average duration for the optimal baseline is around 108 seconds. In the future post, we might look into another strategy to minimize the average duration.\n\n\n\n",
    "preview": "posts/2021-06-27-on-shower-problem/on-shower-problem_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-27T14:33:15+07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-27-coupon-collectors-problem/",
    "title": "Coupon Collector's Problem",
    "description": "Gotta Catch 'Em All!",
    "author": [
      {
        "name": "Farhan Reynaldo",
        "url": "https://weaklyinformative.com"
      }
    ],
    "date": "2021-03-27",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nExperiment\nConclusion\n\nIntroduction\nRecently, I saw a Twitter user collected an entire deck of playing cards on the street while doing his daily walk. He claimed that it took around 6 months to complete the deck. I was wondering, is the story fabricated? So, I run the simulation to estimate how long does it take to complete the deck under several assumptions.\nBut, before we move on to the simulation, the story of completing the deck can be model as coupon collector’s problem, which asks the following question:\n\nGiven n coupons, how many coupons do you expect you need to draw with replacement before having drawn each coupon at least once?\n\nIf we put the statement into our context, the question would be:\n\nGiven 52 unique playing cards, how many cards do you expect you need to pick before having picked each card at least once?\n\nThe simulation of each pick can be represented by geometric distribution, where it calculates the probability of k failures before the first success. In our case, what is the probability that we pick a unique card that we don’t have yet collected? Since in our first try we do not have any cards, then the probability equals \\(52/52 = 1\\) . In our second try, the probability that we pick a new unique cards would be \\(51/52\\), since picking up the previous card is count as failure because we are looking for a new unique cards. We could assume that each day, he pick exactly 1 cards since he did his daily walk.\nGenerally, the probability that the \\(k^{th}\\) trial (out of k trials) is the first success equal\n\\[\nPr(Y = k) = (1-p)^{k-1}p\n\\] for k = 1, 2, 3, and so on. So, on the first day where \\(k=1\\), the equation is written as\n\\[\n\\begin{align}\nPr(Y = 1) &= (1-p)^{1-1}p \\\\\n          &= p = \\frac{52}{52} = 1\n\\end{align}\n\\] In this problem, we are interested in expected days until we find a new unique cards. We could write succinctly the expectation as\n\\[\nt \\sim Geometric(p) \\\\\nE[t] = \\frac{1}{p}\n\\] And by the linearity of expectations, the number of days we need to complete the entire deck is\n\\[\n\\begin{align}\nE[T] &= E[t_1] + E[t_2] + \\dots + E[t_{52}] \\\\\n      &= \\frac{1}{p_1} + \\frac{1}{p_2} + \\dots + \\frac{1}{p_{52}} \\\\\n      &= \\frac{1}{\\frac{52}{52}} + \\frac{1}{\\frac{51}{52}} + \\dots + \\frac{1}{\\frac{1}{52}} \\\\\n      &= \\frac{52}{52} + \\frac{52}{51} + \\dots + \\frac{52}{1} \\approx 236\n\\end{align}\n\\]\nThat is a lot of equations. Let’s calculate the expected days using Python.\nExperiment\n\n\n\n\ndef analytic(n):\n    return sum(n/i for i in range(n, 0, -1))\n\nanalytic(52)\n235.97828543626738\n\nOr, we could actually calculate the expected days by simulation with the following code:\n\nfrom random import random, randint\nfrom statistics import mean\n\ndef expected_days(n_cards):\n    cards = set()\n    days = 0\n    while len(cards) < n_cards:\n        cards.add(randint(1, n_cards))\n        days += 1\n    return days\n\ndays = [expected_days(52) for _ in range(10000)]\nmean(days)\n236.0855\n\nAs expected, the number is similar. So, given that he pick 1 card everyday, the expected number of days until the deck completed is \\(\\approx\\) 236 days, or around 8 months. What if he pick 2 cards everyday? Well, the expected days is equal to\n\ndef expected_days(n_cards, n_pick=2):\n    cards = set()\n    days = 0\n    while len(cards) < n_cards:\n        cards.update(randint(1, n_cards) for _ in range(n_pick))\n        days += 1\n    return days\n\ndays = [expected_days(52, n_pick=2) for _ in range(10000)]\nmean(days)\n118.1981\n\nAbout 4 months. Or to be realistic, there are days when he pick 1 or 2 cards. Assuming the probability is equal, the expected days is equal to\n\ndef expected_days(n_cards):\n    cards = set()\n    days = 0\n    n_pick = randint(1, 2)\n    while len(cards) < n_cards:\n        cards.update(randint(1, n_cards) for _ in range(n_pick))\n        days += 1\n    return days\n\ndays = [expected_days(52) for _ in range(10000)]\nmean(days)\n176.9557\n\nUnder this assumptions, it took around 6 months.\nConclusion\nWell, if you asked me again, is the story fabricated? My answer is I do not know, because I just ran the simulation to see how the result changes as I conditioned on my assumptions. But, it’s been fun to model this problem.\n\n\n\n",
    "preview": "posts/2021-03-27-coupon-collectors-problem/preview.png",
    "last_modified": "2021-06-27T14:33:40+07:00",
    "input_file": {},
    "preview_width": 577,
    "preview_height": 433
  },
  {
    "path": "posts/2020-09-12-approximate-bayesian-computation/",
    "title": "Approximate Bayesian Computation",
    "description": "When all you have is tiny data",
    "author": [
      {
        "name": "Farhan Reynaldo",
        "url": "https://flatprior.xyz"
      }
    ],
    "date": "2020-09-12",
    "categories": [],
    "contents": "\n\nContents\nData Generating Process\nBuilding ABC algorithm\nPrior Sock Distributions\nSocks Simulation\nAcknowledgments\n\n\nThis is a Python port of Rasmus Bååth’s post on Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman, which originally had accompanying code in R. I encourage you to read his blogpost first, and use this blogpost as companion for Python code.\n\nEleven socks. It all started with eleven socks. The problem is stated as follows: Karl Broman picked his first eleven unique socks from his laundry. Given the tiny dataset of eleven unique socks, how many socks does Karl Broman have in his laundry in total?\nData Generating Process\nBefore we start, let’s import python libraries.\nimport numpy as np\nimport pandas as pd\n\n# set seed for reproducibility\nnp.random.seed(12)\nWe start by building a generative model, a simulation of the I’m-picking-out-socks-from-my-laundry process. We have a couple of parameters assign to an arbitrary values:\nn_socks = 18\nn_picked = 11\nBut, there are several socks that didn’t come in pairs (aka singletons). So out of the n_socks let’s say we have:\nn_pairs = 7\nn_odd = 4\nWe create an array of socks, represented as integers, where each pair/singleton is given a unique number.\nsocks = np.repeat(range(n_pairs + n_odd), np.repeat([2, 1], [7, 4]))\nsocks\n\n>> array([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  8,  9, 10])\nFinally we simulate picking out n_picked socks (or at least n_socks if n_picked > n_socks) and counting the number of sock pairs and unique socks. In Python, Pandas is really helpful to create a series of values and count the occurence of each values.\npicked_socks = np.random.choice(socks, size=min(n_picked, n_socks), replace=False)\nsocks_count = pd.Series(picked_socks).value_counts()\ndict(pairs = sum(socks_count == 2), odd = sum(socks_count == 1))\n\n>> {'pairs': 2, 'odd': 7}\nAs Bååth suggested, by figuring out the data generating process, we are almost done in tackling the problem.\nBuilding ABC algorithm\nBååth pointed out how we perform simple ABC rejection algorithm. In general, the algorithm works as follows:\nConstruct a generative model that mimicked the data generating process.\nSample parameters values from the prior distributions and plug the parameters into the generative model and simulate a dataset.\nIf the simulated dataset matches the actual data, add the parameter values to a list. if no, throw them away.\nIterate step 2 and 3 for large number of times building up the list of probable parameter values.\nThe distribution of the probable parameter values represents the posterior information regarding the parameters.\nPrior Sock Distributions\n\nwe can’t afford to use non-informative priors [on this problem], that’s a luxury reserved for the Big Data crowd, we need to use all the information we have.\n\nI can’t agree more with Bååth. With our tiny data, we really need to make a reasonable prior and how we could incorporate our domain knowledge to the data we have. As he mentioned, before seeing any data, we know that n_socks must be positive (no anti-socks) and discrete (socks are not continuous). We choose the negative binomial distribution as prior. It’s reasonable to guesstimate that you would have something like 15 pairs of socks (30 socks) in the laundry. So as a prior for n_socks I’m going to use a negative binomial with mean prior_mu = 30 and standard deviation prior_sd = 15. In Numpy, negative_binomial function is parameterized with the probability p and size n. But, we could transform from mean mu and deviation sd to p and n as follows:\nprior_prob = prior_mu / prior_sd**2\nprior_size = prior_mu * prior_prob / (1 - prior_prob)\n\nprior_mu = 30\nprior_sd = 15\ndata_points = 1000\n\n# number of socks model by negative binomial\nn_socks = np.random.negative_binomial(n=prior_size, p=prior_prob, size=data_points)\nWe create a proportion of pairs prop_pairs distribution as a proxy for prior distribution on n_pairs and n_odd. On prop_pairs, we choose Beta prior distribution that puts most of the probability over the range 0.75 to 1.0. We also round the n_pairs and n_odd as socks are discrete entities.\n# proportion of pairs model by beta distribution\nprop_pairs = np.random.beta(a=15, b=2, size=data_points)\nn_pairs = np.round(np.floor(n_socks / 2) * prop_pairs)\nn_odd = n_socks - n_pairs * 2\nWe create the prior distribution using Seaborn as our choice of data visualization tools.\nprior_plot = (\n    pd.DataFrame(dict(n_socks=n_socks, prop_pairs=prop_pairs, n_pairs=n_pairs, n_odd=n_odd))\n    .melt()\n    .pipe((sns.FacetGrid, 'data'), col=\"variable\", col_wrap=2, sharex=False,\n        sharey=False, height=4, aspect=1.5)\n    .map_dataframe(sns.histplot, \"value\", stat=\"density\", edgecolor='white', bins=20)\n    .add_legend()\n)\nprior_plot\nFigure 1: Prior plotSocks Simulation\nNow we have a generative model, with reasonable priors, let’s push the “simulate” button by running the code from earlier steps together and generates 10,000 samples from the generative model.\ndef simulate_socks(iter=10000):\n    data = []\n\n    # parameter for negative binomial prior distribution\n    prior_mu, prior_sd = 30, 15\n    prior_prob = prior_mu / prior_sd**2\n    prior_size = prior_mu * prior_prob / (1 - prior_prob)\n\n    # parameter for beta prior distribution\n    a, b = 15, 2\n    for _ in range(iter):\n        # number of socks model by negative binomial\n        n_socks = np.random.negative_binomial(n=prior_size, p=prior_prob)\n\n        # proportion of pairs model by beta distribution\n        prop_pairs = np.random.beta(a=a, b=b)\n        n_pairs = int(np.round(np.floor(n_socks / 2) * prop_pairs))\n        n_odd = n_socks - n_pairs * 2\n  \n        # simulate picking socks\n        socks = np.repeat(range(n_pairs + n_odd), np.repeat([2, 1], [n_pairs, n_odd]))\n        picked_socks = np.random.choice(socks, size=min(n_picked, n_socks), replace=False)\n        socks_count = pd.Series(picked_socks).value_counts()\n\n        sample = (sum(socks_count == 2), sum(socks_count == 1), n_socks, n_pairs, n_odd, prop_pairs)\n        data.append(sample)\n    return pd.DataFrame(data, columns=['pairs', 'unique', 'n_socks', 'n_pairs', 'n_odd', 'prop_pairs']) \n\nsocks_sim = simulate_socks()\nsocks_sim.head()\npairs\nunique\nn_socks\nn_pairs\nn_odd\nprop_pairs\n1\n9\n49\n23\n3\n0.95\n0\n11\n59\n25\n9\n0.88\n4\n3\n12\n5\n2\n0.87\n1\n9\n35\n15\n5\n0.85\n2\n7\n26\n11\n4\n0.84\nIn order to turn our simulated samples sock_sim into posterior samples, conditioned on the data, we need to remove the simulated data that doesn’t match the actual data. The data we have is that out of eleven picked socks, eleven were unique and zero were matched, so let’s remove all simulated samples which does not match our data.\npost_samples = socks_sim.query(\"pairs == 0 & unique == 11\")\nAnd now you just performed approximate bayesian computation! After taking account the model and the data, we left with 1,183 remaining samples in post_samples, which represent the information we have about the number of socks in Karl Broman’s laundry. The following plot shows how the the prior sock distributions (blue) shifted into posterior sock distributions (orange) as we incorporate the data observed:\ndef vertical_mean_line(x, **kwargs):\n    plt.axvline(x.mean(), linestyle='--', color=kwargs.get(\"color\", \"g\"))\n\nposterior_plot = (\n    # combine prior and posterior\n    pd.concat([socks_sim.assign(kind='prior'), post_samples.assign(kind='posterior')])\n    .drop(['pairs', 'unique'], axis=1)\n    # tidy up\n    .melt(id_vars='kind', value_vars=['n_socks', 'n_pairs', 'n_odd', 'prop_pairs'])\n    # visualize\n    .pipe((sns.FacetGrid, 'data'), col=\"variable\", hue=\"kind\", col_wrap=2,\n           sharex=False, sharey=False, height=4, aspect=1.5)\n    .map_dataframe(sns.histplot, \"value\", stat=\"density\", edgecolor='white',\n                   common_norm=False, bins=20)\n    .add_legend()\n    .map(vertical_mean_line, 'value') \n)\nposterior_plot\n And the median for each parameter on our post_samples:\npost_samples.median()\npairs\nunique\nn_socks\nn_pairs\nn_odd\nprop_pairs\n0\n11\n44\n19\n6\n0.882561\nThe vertical lines show the median posterior, a “best guess” for the respective parameter. There is a lot of uncertainty in the estimates but Karl Broman told that the actual number of pairs and odd socks is 21 pairs and 3 singleton, which summed to 45 socks. Our “educated” guess using ABC algorithm only missed by 1 sock!\nTo conclude, it’s amazing how we could deduce almost perfectly the number of socks only with tiny data and a little bit of bayes ;)\nSource Code\nYou access and run the full code using Google Colab or Deepnote here.\nAcknowledgments\nThis blogpost made possible entirely by Rasmus Bååth’s post on Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman.\n\n\n\n",
    "preview": "posts/2020-09-12-approximate-bayesian-computation/preview.png",
    "last_modified": "2021-03-27T19:57:22+07:00",
    "input_file": {},
    "preview_width": 577,
    "preview_height": 433
  },
  {
    "path": "posts/2020-09-11-bayesian-review/",
    "title": "Bayesian Review",
    "description": "All about that bayes",
    "author": [
      {
        "name": "Farhan Reynaldo",
        "url": "http://flatprior.xyz"
      }
    ],
    "date": "2020-05-23",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nExperiment\nConclusion\nAcknowledgments\n\nIntroduction\nImagine you are buying a new book in e-commerce, but there is several sellers who sell that book. Because of your past trauma bought stuff online, you look at the seller’s reviews rating as your guide to choose where to buy.\nSeller #1: 100% positive review by 10 reviewers.\nSeller #2: 97% positive review by 100 reviewers.\nSeller #3: 94.3% positive review by 1000 reviewers.\nGiven this situation, which seller would you choose?\nExperiment\nWe are tempted to choose seller #1, as those perfect 100% positive reviews would definitely gave you reassurance to buy from seller #1. But, let’s put in another perspective. Let’s say that each seller have their true positive rate, which means there is a constant that defined how well the seller run their shop measured by positive feedback they got. So, all positive reviews from seller #1 might be happened by chance because we haven’t get more reviewers to review the product. Let me show you an example. Assuming seller #1 have 95% true positive rate, so\nimport numpy as np\nimport pymc3 as pm\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-darkgrid')\nprint('Running with PyMC3 version v.{}'.format(pm.__version__))\n\n>> Running with PyMC3 version v.3.8\nnp.random.seed(12)\n\nSELLER_1_TRUE_RATE = 0.95\nsum(np.random.binomial(10, SELLER_1_TRUE_RATE, 1000) == 10)\n\n>> 577\nBy doing 1000 simulations, we would expect seller #1 to get all positive reviews 577 times. Simply put, there is 57.7% chance that seller #1 get all positive reviews given the true positive rate. We could do this for every value of true positive rate and get the probability for any given number of positive reviews. Basically, we’re trying to find what is the most likely true positive rate for each sellers based on data we have. We will model the probability of true positive rate using beta distribution and model the number of positive reviews using binomial distribution. We use PyMC3 as our tools to model this problem.\n# total reviews, number of positive reviews\nn_1, positive_1 = 10, 10\nn_2, positive_2 = 100, 97\nn_3, positive_3 = 1000, 943\n\nwith pm.Model() as review_1:\n  p_1 = pm.Beta('p_1', alpha=2, beta=2)\n  y = pm.Binomial('y', n=n_1, p=p_1, observed=positive_1)\n  trace_1 = pm.sample(chains=4)\n\nwith pm.Model() as review_2:\n  p_2 = pm.Beta('p_2', alpha=2, beta=2)\n  y = pm.Binomial('y', n=n_2, p=p_2, observed=positive_2)\n  trace_2 = pm.sample(chains=4)\n\nwith pm.Model() as review_3:\n  p_3 = pm.Beta('p_3', alpha=2, beta=2)\n  y = pm.Binomial('y', n=n_3, p=p_3, observed=positive_3)\n  trace_3 = pm.sample(chains=4)\n\naz.plot_trace(trace_1);\naz.plot_trace(trace_2);\naz.plot_trace(trace_3);\nFigure 1: Traceplotp1_summary = pm.summary(trace_1, round_to=3, credible_interval=0.89)\np2_summary = pm.summary(trace_2, round_to=3, credible_interval=0.89)\np3_summary = pm.summary(trace_3, round_to=3, credible_interval=0.89)\nsummary = pd.concat([p1_summary, p2_summary, p3_summary])\nFigure 2: Trace DataframeBased on table above, we could see that seller #1 has lower mean and wider interval compare to seller #2 and seller #3. As we have the parameter distribution, we could calculate the probability that buying from seller #2 would have better experience than seller #1.\nnp.mean(trace_2['p_2'] > trace_1['p_1']) * 100\n\n>> 85.98%\nSo, there is a good chance that you would have better experience from buying from seller #2 rather than seller #1.\nConclusion\nTo conclude, if we only have a relatively small data, we would have a lot of uncertainty around the positive rate, hence the average positive reviews could be misleading. But if we have a lot of reviews, using average of positive reviews would be sufficient.\nAcknowledgments\nThis blogpost is heavily inspired by 3Blue1Brown video on Binomial distributions | Probabilities of probabilities.\n\n\n\n",
    "preview": "posts/2020-09-11-bayesian-review/preview.png",
    "last_modified": "2021-03-27T20:01:34+07:00",
    "input_file": {},
    "preview_width": 707,
    "preview_height": 353
  }
]
